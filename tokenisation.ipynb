{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da7d3288-48f4-4389-aa66-09b5e53bb5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Vanshika\n",
      "[nltk_data]     Garg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Vanshika\n",
      "[nltk_data]     Garg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab') #for sentence/word tokenization\n",
    "nltk.download('wordnet') #wordnet lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f776ab66-0a4c-4b3f-a309-78b5e78d0f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOKENIZATION : breaking paragraph/sentences into words\n",
    "#SENTENCE_TOKENISATION\n",
    "#paragraph-->sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d813fefa-9eb1-449a-a87d-e0d000e0e78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I love animals.', 'Nature is beautiful!', \"Let's protect it.\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "corpus = \"I love animals. Nature is beautiful! Let's protect it.\"\n",
    "sentences = sent_tokenize(corpus)\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "098d829a-f709-4e9b-957a-3d20d146731f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WORD TOKENIZATION\n",
    "#paragraph/sentence-->words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "722b68ee-cec7-47c4-b867-e529c87b3f76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'love',\n",
       " 'animals',\n",
       " '.',\n",
       " 'Nature',\n",
       " 'is',\n",
       " 'beautiful',\n",
       " '!',\n",
       " 'Let',\n",
       " \"'s\",\n",
       " 'protect',\n",
       " 'it',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "(word_tokenize(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45e669af-c2c4-4bda-91a6-9c4044f0a057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'love',\n",
       " 'animals',\n",
       " '.',\n",
       " 'Nature',\n",
       " 'is',\n",
       " 'beautiful',\n",
       " '!',\n",
       " 'Let',\n",
       " \"'\",\n",
       " 's',\n",
       " 'protect',\n",
       " 'it',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "#treats punctuation as different words\n",
    "(wordpunct_tokenize(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b3bf151-3e23-4759-a2a0-5e6d97521390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'love',\n",
       " 'animals.',\n",
       " 'Nature',\n",
       " 'is',\n",
       " 'beautiful',\n",
       " '!',\n",
       " 'Let',\n",
       " \"'s\",\n",
       " 'protect',\n",
       " 'it',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer=TreebankWordTokenizer()\n",
    "tokenizer.tokenize(corpus)\n",
    "\n",
    "# \".\" is not treated as a separate word in mid of sentence "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ddfbc4-1bea-4c06-a0ad-1cf51507e48c",
   "metadata": {},
   "source": [
    "**STEMMING**\n",
    "reducing words to their root/base form\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f58442-8615-4c18-922e-538f6f979509",
   "metadata": {},
   "source": [
    "**PORTER STEMMER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d148ecf6-5225-4d08-ada7-45aa09541b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "playing---->play\n",
      "played---->play\n",
      "player---->player\n",
      "plays---->play\n",
      "history---->histori\n",
      "historical---->histor\n",
      "eat---->eat\n",
      "eaten---->eaten\n",
      "eating---->eat\n"
     ]
    }
   ],
   "source": [
    "words=[\"playing\",\"played\", \"player\", \"plays\",\"history\",\"historical\",\"eat\",\"eaten\",\"eating\"]\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemming=PorterStemmer()\n",
    "\n",
    "for word in words:\n",
    "    print(word+\"---->\"+stemming.stem(word))\n",
    "\n",
    "#Disadvantage: many words cannot be rooted correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b12a364-eb2d-4c1c-b79e-9482bdad0d45",
   "metadata": {},
   "source": [
    "**REGEXP STEMMER** The RegexpStemmer lets you define your own regex rules to chop off word endings (suffixes) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cd20ee7-239e-43be-a915-a661fefa0345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "reg_stemmer=RegexpStemmer('ing$|s$|en$|',min=4)\n",
    "#$--> ending with ing,s,en amd minimum word length=4\n",
    "reg_stemmer.stem('eating')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446fcbb6-9d25-4ab4-b37c-2ab0b89dd68e",
   "metadata": {},
   "source": [
    "**SNOWBALL STEMMER** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91c660a5-e94f-4e11-b7c6-f254976291e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "playing---->play\n",
      "played---->play\n",
      "player---->player\n",
      "plays---->play\n",
      "history---->histori\n",
      "historical---->histor\n",
      "eat---->eat\n",
      "eaten---->eaten\n",
      "eating---->eat\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'fair'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "snow_stemmer=SnowballStemmer('english')\n",
    "for word in words:\n",
    "    print(word+\"---->\"+snow_stemmer.stem(word))\n",
    "\n",
    "snow_stemmer.stem('fairly')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccf1028-7ce9-4072-be0c-e9589149f9eb",
   "metadata": {},
   "source": [
    "**LEMETIZATION** : returning a word to its base/real word form\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fd8106f-16e9-4d5d-9b4d-2cd94d099b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "playing---->play\n",
      "played---->play\n",
      "player---->player\n",
      "plays---->play\n",
      "history---->history\n",
      "historical---->historical\n",
      "eat---->eat\n",
      "eaten---->eat\n",
      "eating---->eat\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('going',pos='v')\n",
    "\n",
    "'''POS :\n",
    "NOUN-n\n",
    "VERB-v\n",
    "ADJECTIVE-a\n",
    "ADVERB-r'''\n",
    "\n",
    "for word in words:\n",
    "    print(word+ \"---->\"+lemmatizer.lemmatize(word,pos='v'))\n",
    "## Q&a,CHATBOTS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ece4000-73a8-4403-9510-a8401fe4febf",
   "metadata": {},
   "source": [
    "**STOPWORDS** : #those words which do not carry much meaning in a text and are generally filtered before text processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bbfbdc7-849f-4b9a-a71d-8d054093d581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3eb9fea6-1506-4189-9811-b7515238de15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Vanshika\n",
      "[nltk_data]     Garg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1dd16237-15e8-44b0-9c7c-33bd56f14e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bbe48d8-35bf-41df-96b5-301bf85bdb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph=\"\"\"Good morning everyone. Today, I want to talk about the importance of innovation in education. \n",
    "We believe that by embracing new technologies and creative thinking, we can transform the way students learn and grow. \n",
    "It's not just about books anymore; it's about experiences, interactions, and real-world problem-solving.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee429076-26eb-45ad-8817-bd9c587939cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "560abe7e-3454-4eb7-876c-f7ca69d12a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "816c2ec2-3506-46bf-b35b-32f5c33d43ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b030962f-744a-40a4-b279-7578c280b607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good morning everyone.', 'Today, I want to talk about the importance of innovation in education.', 'We believe that by embracing new technologies and creative thinking, we can transform the way students learn and grow.', \"It's not just about books anymore; it's about experiences, interactions, and real-world problem-solving.\"]\n"
     ]
    }
   ],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3c1d488-6f67-40ba-8e29-be25b347b5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stemmer=SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e4daabf-b84f-4f07-b627-9de1af8d5808",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[stemmer.stem(word)for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i]=' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ce5b5a4-012d-4f48-8a21-59cbe3753f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good morn everyon .',\n",
       " 'today , i want talk import innov educ .',\n",
       " 'we believ embrac new technolog creativ think , transform way student learn grow .',\n",
       " \"it 's book anymor ; 's experi , interact , real-world problem-solv .\"]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ed221e5-462d-46e5-89b9-b16fd19dfe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmitizer=WordNetLemmatizer()\n",
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[lemmitizer.lemmatize(word,pos='v')for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i]=' '.join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3dee3d74-ba2c-4c6a-8a4c-3cd53b740087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['good morn everyon .',\n",
       " 'today , want talk import innov educ .',\n",
       " 'believ embrac new technolog creativ think , transform way student learn grow .',\n",
       " \"'s book anymor ; 's experi , interact , real-world problem-solv .\"]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec44338-b49b-4af1-84e9-507613a2658c",
   "metadata": {},
   "source": [
    "**PARTS OF SPEECH TAGGS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea18e2e2-99bf-4697-bff4-0db4cb9db0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Vanshika Garg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Good', 'JJ'), ('morning', 'NN'), ('everyone', 'NN'), ('.', '.')]\n",
      "[('Today', 'NN'), (',', ','), ('I', 'PRP'), ('want', 'VBP'), ('talk', 'NN'), ('importance', 'NN'), ('innovation', 'NN'), ('education', 'NN'), ('.', '.')]\n",
      "[('We', 'PRP'), ('believe', 'VBP'), ('embrace', 'JJ'), ('new', 'JJ'), ('technologies', 'NNS'), ('creative', 'JJ'), ('think', 'NN'), (',', ','), ('transform', 'VB'), ('way', 'NN'), ('students', 'NNS'), ('learn', 'VBP'), ('grow', 'NN'), ('.', '.')]\n",
      "[('It', 'PRP'), (\"'s\", 'VBZ'), ('book', 'NN'), ('anymore', 'RB'), (';', ':'), (\"'s\", 'POS'), ('experience', 'NN'), (',', ','), ('interactions', 'NNS'), (',', ','), ('real-world', 'JJ'), ('problem-solving', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "sentences=nltk.sent_tokenize(paragraph)\n",
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[lemmitizer.lemmatize(word,pos='v')for word in words if word not in set(stopwords.words('english'))]\n",
    "    pos_tag=nltk.pos_tag(words)\n",
    "    print(pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9120bd64-ee39-4537-8893-9208b62d1b76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good morning everyone.',\n",
       " 'Today, I want to talk about the importance of innovation in education.',\n",
       " 'We believe that by embracing new technologies and creative thinking, we can transform the way students learn and grow.',\n",
       " \"It's not just about books anymore; it's about experiences, interactions, and real-world problem-solving.\"]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99ce331e-8f82-4c43-a4eb-3c23ac9fee26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Taj', 'Mahal', 'is', 'a', 'beautiful', 'monument']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Taj Mahal is a beautiful monument\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2e4ddaa-100d-490f-8290-dc433311f9ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Taj', 'NNP'), ('Mahal', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('beautiful', 'JJ'), ('monument', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.pos_tag(\"Taj Mahal is a beautiful monument\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ea18e3-f750-4503-b23d-aa599e144232",
   "metadata": {},
   "source": [
    "**NAMED ENTITY ORGANIZATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55fe0d81-4380-4f5d-9bbd-6e90290a0579",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     C:\\Users\\Vanshika Garg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n",
      "[nltk_data] Downloading package words to C:\\Users\\Vanshika\n",
      "[nltk_data]     Garg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    " import nltk\n",
    " nltk.download('maxent_ne_chunker_tab')\n",
    "nltk.download('words')\n",
    "sentences=\"Taj Mahal is a beautiful monument built in 1913 by shehnshah\"\n",
    "words=nltk.word_tokenize(sentences)\n",
    "tag_elements=nltk.pos_tag(words)\n",
    "\n",
    "nltk.ne_chunk(tag_elements).draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71c6065-8804-4770-aafc-050ff252bf92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1cb5b4-42b8-42ed-be63-8ad7483b1d52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84e3965-f890-4263-9fd4-29ca8a75cb97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
